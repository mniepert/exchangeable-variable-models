import numpy as np
from operator import mul
import operator
import scipy
from collections import Counter
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform

# computes the binomial coefficient
def  n_take_k(n,r):
  
    if r > n-r:  # for smaller intermediate values
        r = n-r
    return int( reduce( mul, range((n-r+1), n+1), 1) /
      reduce( mul, range(1,r+1), 1) )


# class representing one mixture component
class MComponent:
	splitNr = 2
	c = Counter([])
	partition = array([])
	smooth = 1.0
	nk = dict()

	# part is the partition in form of an array [0, 0, 1, 0, 1, 2, 3, 1] indicating column membership on (here: 4) partitions
	def __init__(self, data):
		
		# get dimensions of the data matrix
		m,n = data.shape

		#build an array of arrays that stores the probs
		configs = zeros(m, dtype=int)

		# stores the possible binomial coefficients (caching)
		self.nk = dict()

		#compute the number of all possible configurations
		prod = n+1
		
		# go through all rows of the data table and store number of configurations
		for i in arange(m):
			configs[i] = count_nonzero(data[i])


		# count the frequencies of the different configurations
		self.c = Counter(configs)

		# number of configurations that have no occurence in the training data
		diff = prod - len(self.c)

		#print "diff: ",diff
		#print self.c

		#  the normalization constant for the probabilities of the block configurations
		self.smooth = float(sum(self.c.values()))

		# perform Laplacian smoothing -> add 1 count to each possible configuration
		# we do this only for the fully exchangeable component (splitNr == 1)
		self.smooth += diff
		for i in list(self.c):
			self.c[i] += 1
			self.smooth += 1

	# returns the probability of one particular configuration (here: conditional probability)
	def prob(self, data_point):

		# iterate over the number of blocks
		configs_test = count_nonzero(data_point)

		# convert the array to a tuple (required for the look-up in the Counter structure)
		#x = tuple(configs_test)
		
		# look up the probability of the given block configuration		
		if self.c[configs_test] > 0:
			currProb = float(self.c[configs_test])/self.smooth
		else:
			# if the configuration probability is zero and we are fully exchangeable, apply smoothing
			currProb = 1.0/self.smooth

		# normalize by the number of configuration represented by this particular block configuration
		nvalue = len(data_point)
		kvalue = configs_test
		cnk = tuple([nvalue, kvalue])
		tst = self.nk.get(cnk, False)
		if tst:
			currProb = currProb / tst
		else:
			tst = n_take_k(nvalue, kvalue)
			self.nk.setdefault(cnk, tst)
			currProb = currProb / tst

		return currProb


# the name of the data set	
dataSetName = "bnetflix"

# load the training data
data = numpy.loadtxt(open(dataSetName+".ts.data","rb"),dtype=int,delimiter=",")

# get the dimensions of the trainging data matrix
m,n = data.shape

# compute the priors from the training data: prob(x=1)
prior = zeros(n,dtype=float)
for i in arange(n):
	prior[i] = mean(data[:,i])


resultNB = zeros(n, dtype=float)
resultNBE = zeros(n, dtype=float)
resultBL = zeros(n, dtype=float)

resultCLLNB = zeros(n, dtype=float)
resultCLLNBE = zeros(n, dtype=float)


# iterate over all variables in the data set
for c in arange(0,n):

	print "Processing column ",c," out of ",n,"" 
	print "1-Prior: ",1.0-prior[c]

	# set current position
	position = c

	# get the matrix where the position column's value is 0
	data0 = data[data[:,position]==0]

	# get the matrix where the position column's value is 1
	data1 = data[data[:,position]==1]

	# compute the marginal probabilities of the involved variables
	ms0 = zeros(n,dtype=float)
	ms1 = zeros(n,dtype=float)
	ms1t = zeros(n,dtype=float)
	for i in arange(n):
		ms0[i] = (float(sum(data0[:,i]==1)) + 1.0) / (float(data0.shape[0])+2.0)
		ms1[i] = (float(sum(data1[:,i]==1)) + 1.0) / (float(data1.shape[0])+2.0)
		#ms1t[i] = mean(data0[:,i])

	#print ms0
	#print ms0
	#print ms1t

	
	# this is used to extract the submatrix (the training data minus the target column)
	target = ones(n, dtype=int)
	target[position] = 0

	# here we can train different (partial) exchangeable models #

	#cut out the part of the training data that is not the class variable (i.e., the features)
	comp0Data = data0[:,target==1]
	comp1Data = data1[:,target==1]

	# a mixture component for the ith row having value '0'
	comp0 = MComponent(comp0Data)
	# create a mixture component for the ith row having value '1'
	comp1 = MComponent(comp1Data)

	cd0 = array_split(comp0Data, 5)
	cd1 = array_split(comp1Data, 5)
	
	comp0t = array([])
	comp1t = array([])

	for i in arange(5):
		tmpC = MComponent(cd0[i])
		comp0t = append(comp0t, tmpC)
		tmpC = MComponent(cd1[i])
		comp1t = append(comp1t, tmpC)



	############################################################################
	#################### EVALUATION ############################################
	############################################################################

	# load test data
	data_test = numpy.loadtxt(open(dataSetName+".test.data","rb"),dtype=int,delimiter=",")

	# dimensions of test data
	mt,nt = data_test.shape

	# compute the accuracy of the naive Bayes model (independence of variables given the class) on the test data
	cllSum = 0.0
	correctCounter = 0
	for i in arange(mt):

		pr0 = 1.0
		pr1 = 1.0
		for j in arange(nt):
			if j != position:
				pr0 = pr0*((1.0-data_test[i][j])*(1.0-ms0[j])+data_test[i][j]*ms0[j])
				pr1 = pr1*((1.0-data_test[i][j])*(1.0-ms1[j])+data_test[i][j]*ms1[j])

		pr0 = pr0 * (1.0-prior[position])
		pr1 = pr1 * prior[position]

		if pr0 >= pr1 and data_test[i][position]==0:
			correctCounter += 1
		elif pr0 < pr1 and data_test[i][position]==1:	
			correctCounter += 1

		if data_test[i][position]==0:
			cllSum = cllSum + log(pr0/(pr0+pr1))
			#print pr0/(pr0+pr1), "        ",pr0,"            ",pr1
		else:
			cllSum = cllSum + log(pr1/(pr0+pr1))
			#print pr1/(pr0+pr1), "        ",pr1,"            ",pr0


	resultNB[position] = 1.0 - (float(correctCounter) / float(mt))
	resultCLLNB[position] = cllSum/float(mt)
	print "Independent model; error: ",1.0-float(correctCounter) / float(mt),"; CLL: ",cllSum/float(mt)


	# compute the accuracy for the exchangeable Naive Bayes model (exchangeability of variables given the class) on the test data
	cllSum = 0.0
	cllSum2 = 0.0
	correctCounter = 0
	for i in arange(mt):
		
		pr0 = comp0.prob(data_test[i,target==1])
		pr1 = comp1.prob(data_test[i,target==1])

		pr0 = pr0 * (1.0-prior[position])
		pr1 = pr1 * prior[position]

		dtSplit = array_split(data_test[i,target==1], 5)

		pr0t = ones(5, dtype=float)
		pr1t = ones(5, dtype=float)
		for j in arange(5):
			pr0t[j] = comp0t[j].prob(dtSplit[j])
			pr1t[j] = comp1t[j].prob(dtSplit[j])
		
		pr0t = (1.0-prior[position]) * prod(pr0t)
		pr1t = prior[position] * prod(pr1t)

		if pr0 >= pr1 and data_test[i][position]==0:
			correctCounter += 1
		elif pr0 < pr1 and data_test[i][position]==1:	
			correctCounter += 1

		if data_test[i][position]==0:
			cllSum = cllSum + log(pr0/(pr0+pr1))
			cllSum2 = cllSum2 + log(pr0t/(pr0t+pr1t))
			#print pr0/(pr0+pr1), "        ",pr0,"            ",pr1
		else:
			cllSum = cllSum + log(pr1/(pr0+pr1))
			cllSum2 = cllSum2 + log(pr1t/(pr0t+pr1t))
			#print pr1/(pr0+pr1), "        ",pr1,"            ",pr0

	
	resultNBE[position] = 1.0 - (float(correctCounter) / float(mt))
	resultCLLNBE[position] = cllSum/float(mt)
	print "Exchangeable model; error: ",1.0-float(correctCounter) / float(mt),"; CLL: ",cllSum/float(mt),";  advanced: ",cllSum2/float(mt)


	# compute the accuracy of the majority baseline
	if prior[c] < 0.5:
		resultBL[position] = prior[c]
	else:
		resultBL[position] = (1.0-prior[c])


print dataSetName
#print "Baseline; mean: ",mean(resultBL), "  standard Deviation: ",std(resultBL)
#print "Independent model; mean: ",mean(resultNB), "  standard Deviation: ",std(resultNB)
#print "Exchangeable model: mean: ",mean(resultNBE), "  standard Deviation: ",std(resultNBE)
#print "Mean of differences (BL<->NB): ",mean(resultBL - resultNB)
#print "StdDev of differences(BL<->NB): ",std(resultBL - resultNB)/(sqrt(len(resultNB)))
print "Independent model (error); mean: ",mean(resultNB), "  standard Deviation: ",std(resultNB)
print "Exchangeable model (error): mean: ",mean(resultNBE), "  standard Deviation: ",std(resultNBE)

print "CLL..."

print "Independent model (CLL); mean: ",mean(resultCLLNB), "  standard Deviation: ",std(resultCLLNB)
print "Exchangeable model (CLL): mean: ",mean(resultCLLNBE), "  standard Deviation: ",std(resultCLLNBE)
print "Mean of differences (CLL: NB<->NBE): ",mean(-resultCLLNB + resultCLLNBE)
print "StdDev of differences(CLL: NB<->NBE): ",std(-resultCLLNB + resultCLLNBE)/(sqrt(len(resultCLLNB)))

