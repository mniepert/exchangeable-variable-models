import numpy as np
from operator import mul
import operator
import scipy
from collections import Counter
from scipy.cluster.hierarchy import linkage, fcluster
from scipy.spatial.distance import squareform

# computes the binomial coefficient
def  n_take_k(n,r):
  
    if r > n-r:  # for smaller intermediate values
        r = n-r
    return int( reduce( mul, range((n-r+1), n+1), 1) /
      reduce( mul, range(1,r+1), 1) )


# class representing one mixture component
class MComponent:
	splitNr = 2
	splitNumbers = [0]
	partition = array([])
	smooth = 1.0
	nk = dict()
	sampleWeights = dict()

	# part is the partition in form of an array [0, 0, 1, 0, 1, 2, 3, 1] indicating column membership on (here: 4) partitions
	def __init__(self, data, part, weights):
		
		# get dimensions of the data matrix
		m,n = data.shape

		# compute number of blocks
		self.splitNr = len(unique(part))
		# copy the partition indicator array to the class variable "partition"
		self.partition = part
		# the integers used in part to index the blocks (e.g.: [0, 2, 3])
		self.splitNumbers = unique(part)
		
		#build an array of arrays that stores the probs
		configs = zeros((m, self.splitNr), dtype=int)

		# stores the possible binomial coefficients (caching)
		self.nk = dict()
		# stores the accumulated weights 
		self.sampleWeights = dict()

		#compute the number of all possible configurations
		prod = 1.0
		for i in self.splitNumbers:
			blockSize = len(self.partition[part==i])
			#print "blockSize: ",blockSize
			prod = prod * (blockSize+1)

		dummyCounter = 0

		# go through all rows of the data table and store number of configurations
		for i in arange(m):
			for j in arange(self.splitNr):
				configs[i][j] = count_nonzero(data[i][part==self.splitNumbers[j]])
			configTuple = tuple(configs[i])
			#print configTuple
			# configs[i] stores the count for each block
			sWeight = self.sampleWeights.get(configTuple, -1.0)
			#print sWeight
			if sWeight > 0:
				sWeight += weights[i]
				#print sWeight
				self.sampleWeights[configTuple] = sWeight
			else:
				self.sampleWeights[configTuple] = weights[i]


		#print dummyCounter
		# number of configurations that have no occurence in the training data
		diff = prod - len(self.sampleWeights)

		#print "diff: ",diff

		#  the normalization constant for the probabilities of the block configurations
		self.smooth = float(sum(self.sampleWeights.values()))

		#print self.smooth

		# perform Laplacian smoothing -> add 1 count to each possible configuration
		# we do this only for the fully exchangeable component (splitNr == 1)
		if self.splitNr == 1:
			self.smooth += diff
			for i in list(self.sampleWeights):
				self.sampleWeights[i] += 1.0
				self.smooth += 1.0

		#print self.sampleWeights

	# returns the probability of one particular configuration (here: conditional probability)
	def prob(self, data_point):

		# the vector representing the projection of the data point to the exchangeable blocks
		configs_test = zeros((self.splitNr,), dtype=int)

		# iterate over the number of blocks
		for i in arange(self.splitNr):
			configs_test[i] = count_nonzero(data_point[self.partition==self.splitNumbers[i]])

		# convert the array to a tuple (required for the look-up in the Counter structure)
		x = tuple(configs_test)
		
		# look up the probability of the given block configuration		
		if self.sampleWeights.get(x, -1.0) > 0:
			currProb = float(self.sampleWeights[x])/self.smooth
		else:
			# if the configuration probability is zero and we are fully exchangeable, apply smoothing
			if self.splitNr == 1:
				currProb = 1.0/self.smooth
			# if the configuration probability is zero and we are *not* fully exchangeable, return 0.0
			else:
				return 0.0

		# normalize by the number of configuration represented by this particular block configuration
		for i in arange(self.splitNr):
			nvalue = len(self.partition[self.partition==self.splitNumbers[i]])
			kvalue = configs_test[i]
			cnk = tuple([nvalue, kvalue])
			tst = self.nk.get(cnk, False)
			if tst:
				currProb = currProb / tst
			else:
				tst = n_take_k(nvalue, kvalue)
				self.nk[cnk] = tst
				currProb = currProb / tst

		return currProb


# the name of the data set	
dataSetName = "nltcs"

# load the training data
data = numpy.loadtxt(open(dataSetName+".ts.data","rb"),dtype=int,delimiter=",")

# get the dimensions of the trainging data matrix
m,n = data.shape

# compute the priors from the training data: prob(x=1)
ms = zeros(n,dtype=float)
for i in arange(n):
	ms[i] = mean(data[:,i])

# the number of mixture components (latent variable values)
numComponents = 10

initData = np.random.randint(2, size=(m, n))
initData = array_split(initData, numComponents)

# comp are the mixture components
comp = array([])
# initialization of the components
assign = zeros((numComponents, n),dtype=int)
for j in arange(numComponents):
	# this indicates that we are using the fully exchangeable model
	#for k in arange(j):
	if j > 0:
		assign[j][j] = 1
	# create a mixture component for the ith row having value '0'
	compTemp = MComponent(initData[j], assign[j], ones(m, dtype=float))
	#savetxt(dataSetName+str(j)+".shuffle.data", data, fmt='%c', delimiter=',')   # X is an array
	comp = append(comp, compTemp)

print assign


# class probabilities initialized to uniform probabilities
latentProb = ones(numComponents, dtype=float)
latentProb = latentProb/sum(latentProb)

print latentProb

for c in arange(10):
	print c
	# iterate over the training samples (all of them) an compute probability
	compPr = zeros(numComponents, dtype=float)
	weights = zeros((numComponents, m), dtype=float)
	# the E step
	for i in arange(m):
		probSum = 0.0
		for j in arange(numComponents):
			# probability (unnormalized) of the data point i for the component j
			prob = latentProb[j] * comp[j].prob(data[i])
			weights[j][i] = prob
			# the sum of the probabilites (used for normalization)
			probSum += prob
			#print weights[j][i]
		
		#print " "
		#print weights
		#print probSum

		for j in arange(numComponents):
			# normalize the probabilities
			weights[j][i] = weights[j][i] / probSum
			# aggregate the normalized probabilities
			compPr[j] += weights[j][i]
			#print weights[j][i],

		#print " ---- "

	# the M step
	# update the class priors
	latentProb = compPr/sum(compPr)

	# update the parameters of the mixture components
	# comp are the mixture components
	comp = array([])
	# run inference in the components and compute the new probabilities
	for j in arange(numComponents):
		# this indicates that we are using the fully exchangeable model
		#assign = zeros(n,dtype=int)
		# create a mixture component for the ith row having value '0'
		compTemp = MComponent(data, assign[j], weights[j])
		comp = append(comp, compTemp)
		#print weights[j]

	print latentProb
	#print "---"


# load test data
data_test = numpy.loadtxt(open(dataSetName+".test.data","rb"),dtype=int,delimiter=",")

# dimensions of test data
mt,nt = data_test.shape

# compute the log-likelihood of the test data for the partial exchangeable model
testCounter = 0
sumn = 0.0
for x in data_test:
	testCounter += 1
	prob = 0.0
	for j in arange(numComponents):
		if latentProb[j] > 0.0:
			prob = prob + latentProb[j] * comp[j].prob(x)
	sumn = sumn + log(prob)

print testCounter
print "Mixture of smoothed partially exchangeable sequences: ",sumn / len(data_test)


